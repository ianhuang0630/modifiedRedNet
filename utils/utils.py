import numpy as np
from torch import nn
import torch
import os

#med_frq = [0.382900, 0.452448, 0.637584, 0.377464, 0.585595,
#           0.479574, 0.781544, 0.982534, 1.017466, 0.624581,
#           2.589096, 0.980794, 0.920340, 0.667984, 1.172291,
#           0.862240, 0.921714, 2.154782, 1.187832, 1.178115,
#           1.848545, 1.428922, 2.849658, 0.771605, 1.656668,
#           4.483506, 2.209922, 1.120280, 2.790182, 0.706519,
#           3.994768, 2.220004, 0.972934, 1.481525, 5.342475,
#           0.750738, 4.040773]

# changing med_frq for minos dataset
# med_frq = [0.0031364030687582515, 0.45708582444648566, 0.005121191195886318, 
#         0.004184337215968313, 0.016647125916371342, 0.009691772418525467, 
#         0.000857126822970954, 0.0018281004349506636, 0.06766117386734764, 
#         0.024107198889496907, 0.0005816250445156695, 7.472253708915294e-05, 
#         7.468250013028976e-05, 0.0023938654150163297, 0.012736504423337503, 
#         0.212998722753153, 0.01024809003348447, 0.06684131127419046, 
#         0.0008024689096049614, 0.0009034828354092836, 0.01789152752805573, 
#         0.0026299226458811062, 0.0007897290135761934, 0.015659329317151343, 
#         0.014464669216003057, 0.0016607439111250087, 0.0, 0.0, 
#         0.00012034024086234452, 0.0026988262520846362, 0.0, 
#         0.0014436173758772845, 4.6569090490584394e-05, 0.0020672846201619066, 
#         0.00020085524372871934, 0.001088800346136474, 0.00030721444283753735, 
#         0.0, 4.341634962823987e-06, 9.679443749565701e-06, 
#         0.00010595136500243208, 2.5107923354874575e-08, 0.001084425460140018, 
#         0.03975041774155722]
# scaled

# med_frq = [0.31364030687582517, 45.70858244464856, 0.5121191195886319, 
#         0.41843372159683134, 1.6647125916371341, 0.9691772418525467, 
#         0.0857126822970954, 0.18281004349506635, 6.766117386734765, 
#         2.4107198889496906, 0.05816250445156695, 0.007472253708915294, 
#         0.007468250013028976, 0.23938654150163297, 1.2736504423337502, 
#         21.299872275315302, 1.024809003348447, 6.684131127419047, 
#         0.08024689096049614, 0.09034828354092836, 1.7891527528055728, 
#         0.26299226458811065, 0.07897290135761934, 1.5659329317151343, 
#         1.4464669216003057, 0.16607439111250086, 0.0, 0.0, 
#         0.012034024086234451, 0.2698826252084636, 0.0, 0.14436173758772844, 
#         0.0046569090490584395, 0.20672846201619066, 0.020085524372871935, 
#         0.10888003461364741, 0.030721444283753736, 0.0, 0.00043416349628239875, 
#         0.0009679443749565701, 0.010595136500243208, 2.5107923354874577e-06, 
#         0.1084425460140018, 3.9750417741557222]


# TODO: write new code to generate the following numbers
med_frq = [0.09275865695742824, 0.6915455654970292, 4.186289237229926, 0.07988118674726077, 0.3219049876522853, 3.613991360857743, 0.1170961873756432, 0.6087556387466655, 1.207442353411119, 0.12230684629427109, 1.039718116076704, 103.58477269586845, 1.823004136542376, 0.23142793899865, 0.21176445662905352, 0.14404195115690027, 0.3020704094761498, 1.1887811252176506, 7.325597076183403, 102.06167460857726, 3.838260593524075, 0.4318493683355512, 3.1617432454090713, 1.1038966967462245, 6.295130287278622, 3.3943228949703874, 2.525555634165816, 0.14275042350417716, 0.3234779479314953, 0.2730630518728115, 0.12491460103938451, 0.834899608412668, 4.2714944244696555, 1.300869568235337, 7.70247110197791, 0.47463903922765416, 0.805476193290304, 0.3077697131082424, 0.27684281392953547, 0.7818603370484171, 10.109272594870136, 0.8367896179743173, 0.7609993939560055, 1.079126096194485, 2.6187395418865997, 1.6721755772868412, 5.860248592870544, 0.35408838570747725, 0.1473378119152817, 0.31279868244868597, 8.840755242116188, 0.7120049921072277, 0.08232248091844481, 1.832948638326381, 536.6091624910523, 0.6202439795271656, 2.0457846318426562, 4.8765832048554865, 2.115096282711434, 0.8570091000548747, 0.3142615675034617, 2.0788941671727716, 0.3984296521806607, 1.793803406020957, 0.18549853831775148, 49.65180818651477, 0.452539776061198, 0.1678535458818904, 0.1922178295913349, 0.13848193270969547, 5.664694414218353, 1.0, 2.216357411480877, 0.17182533315607795, 0.9846789797020126, 0.2692385325186725, 2.386129033284845, 0.9727651755509447, 0.44554827942405373, 0.9939789229153628, 6.009499531035809, 0.8727282252478273, 0.3685426390583627, 0.9341532043006465, 0.8909959422549842, 0.2790202989023325, 0.19558406123078875, 0.17803151566046, 1.2598618533830794, 0.47866347150308697, 2.1809636303862727, 1.02287556728405, 0.0199519864994334, 6.866937810876914, 0.846387647242793, 2.306657722829248, 1.1407226495628953, 2.1886172737862717, 0.29593429498567, 0.8269924288640869, 1.1384480574901517, 1.556852165678114, 3.2330872141668032, 0.14160762279827513, 0.5479522045843777, 172.7686102788661, 0.18942983525654064, 0.5630935015105605, 16.437737090231337, 11.030001177093755, 0.13976176981647068, 1.684727303781879, 4.698895547086551, 2.0022943866321934, 0.20487075790726797, 0.2155046965025042, 30.699168680126135, 69.23829315599889, 1.527032174612713, 0.07152765373746811, 0.033829607837332604, 0.18138127369967538, 12.17110988440057, 39.695154884829236, 1.2497611819368522, 3.3785965386695516, 17.023412662367154, 0.20910088985511613, 0.33164057108274264, 0.23853530667338702, 0.4298599021516993, 0.3212647402173298, 10.838473216222079, 0.4056722674296216, 0.11357785007462588, 2.0434870313075004, 49976.200000000004, 0.32257777181083297, 1.807549025990466, 0.33846311523520395, 0.8238840122256672, 0.48106122590420014, 2.7760956912992762, 0.47827379604221787, 6.526921134657914, 341.9904197080292, 5.922286301153421, 2.4298744615264933, 3.993538041914828, 7.45943122114313, 9.202025409685142, 3.172581764626219, 1341.0429338103756, 0.855552727716789, 3.127121719651431, 13.97336340590516, 0.36740001960399926, 764.9418367346939, 0.77263109290727, 0.1559709456278105, 0.9788915759563755, 27.16491520510219, 0.14868582440529082, 6.775025305473212, 0.6381194941303355, 0.7049638040282948, 1.3857693203538908, 0.07671141788460546, 23.03616864359904, 0.30166392558310234, 1.1863598597846128, 1.330196714446428, 0.48889994691278965, 1.016693927039849, 1.3019382066378367, 6.923253816529521, 6.265927213761514, 0.08220736033884685, 29.314992961051153, 0.3712730577505491, 6.770191552195942, 1.2832573283211108, 0.25249593203749093, 5.4883920138812625, 29.74537735100389, 1.5850565817517508, 2.798700038080447, 0.10845982998999815, 0.5409015278004263, 0.6364060064587769, 4.917046006113159, 1.458367134928185, 0.3040357456742486, 0.6116049793668586, 49.48138613861386, 17.6482096193234, 0.1133441127037915, 225.38875526157548, 1.4115178218381068, 0.8081341322219612, 0.03292275502505451, 0.28508858497917294, 0.6225117773037054, 0.42988677124580305, 32.590339970437356, 1.0716855921166435, 1.2867750933356221, 2.814260454325326, 20.666124496884823, 36.85560471976402, 1.00977251787475, 0.4238628563415768, 2.5244329948982167, 0.5438244714229233, 0.27876495170607307, 0.8600699627240259, 0.18326278023222387, 1.05542047561592, 0.15687282666902716, 0.47856813171870544, 0.9088708025028887, 1.025140203402625, 0.07103543655717735, 0.7810424693529264, 1.157074325721818, 0.35000672332939275, 0.3993472091780664, 4488.880239520959, 4.4815808981778185, 28.25003768465481, 0.7913712126730366, 0.6660426576918361, 1.3289539697031476, 1.0560970043278335, 2.182138116529272, 1.593188532080845, 1.444487906768818, 0.5946970598454956, 0.20673470834180202, 2.3852483271446436, 0.12756628182294966, 0.04189797444750216, 3.587907301757476, 2.3253540874377285, 0.25041020022313826, 15.475382423979688, 0.5695250178536156, 0.20113661426217203, 1.3648335202528512, 0.36614031317462953, 569.637537993921, 2.667759189469077, 0.26457319020754183]


model_urls = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',
    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',
    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',
    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',
}

# TODO: may need ot come up with more colors
# label_colours = [(0, 0, 0),
#                  # 0=background
#                  (148, 65, 137), (255, 116, 69), (86, 156, 137),
#                  (202, 179, 158), (155, 99, 235), (161, 107, 108),
#                  (133, 160, 103), (76, 152, 126), (84, 62, 35),
#                  (44, 80, 130), (31, 184, 157), (101, 144, 77),
#                  (23, 197, 62), (141, 168, 145), (142, 151, 136),
#                  (115, 201, 77), (100, 216, 255), (57, 156, 36),
#                  (88, 108, 129), (105, 129, 112), (42, 137, 126),
#                  (155, 108, 249), (166, 148, 143), (81, 91, 87),
#                  (100, 124, 51), (73, 131, 121), (157, 210, 220),
#                  (134, 181, 60), (221, 223, 147), (123, 108, 131),
#                  (161, 66, 179), (163, 221, 160), (31, 146, 98),
#                  (99, 121, 30), (49, 89, 240), (116, 108, 9),
#                  (161, 176, 169), (80, 29, 135), (177, 105, 197),
#                  (139, 110, 246)]
label_colours = [(0, 0, 0), 
                (112, 142, 254), (101, 238, 135), (172, 175, 122), 
                (249, 251, 122), (209, 185, 146), (183, 215, 202), 
                (138, 223, 235), (128, 187, 192), (126, 141, 199), 
                (234, 117, 111), (131, 254, 156), (132, 244, 173), 
                (191, 204, 162), (250, 136, 118), (209, 214, 180), 
                (230, 148, 116), (152, 206, 120), (176, 106, 168), 
                (185, 136, 171), (181, 201, 124), (167, 161, 209), 
                (156, 176, 100), (106, 193, 237), (232, 202, 109), 
                (132, 174, 241), (166, 209, 105), (206, 120, 208), 
                (144, 150, 197), (112, 185, 142), (205, 230, 229), 
                (140, 186, 179), (189, 176, 205), (121, 240, 136), 
                (194, 147, 217), (141, 185, 194), (198, 244, 151), 
                (252, 216, 191), (155, 248, 253), (125, 219, 211), 
                (117, 139, 210), (106, 183, 190), (132, 161, 239), 
                (179, 218, 180), (223, 188, 166)]

label_colours = [(0,0,0), # for the background
                (112, 127, 176), (238, 59, 36), (71, 163, 224), (83, 70, 203), (88, 187, 226), (124, 80, 190), (131, 201, 102), (227, 117, 101), (160, 56, 46), (64, 55, 64), (176, 225, 104), (225, 197, 52), (242, 30, 64), (50, 150, 148), (200, 202, 182), (133, 195, 83), (118, 224, 224), (166, 186, 239), (192, 125, 74), (235, 184, 41), (69, 157, 124), (132, 118, 248), (145, 102, 87), (187, 149, 211), (249, 177, 65), (249, 30, 165), (45, 80, 247), (36, 110, 144), (205, 165, 166), (46, 105, 33), (220, 87, 215), (197, 85, 160), (77, 134, 93), (197, 142, 140), (195, 248, 201), (231, 89, 58), (178, 170, 104), (237, 185, 217), (119, 45, 243), (79, 142, 35), (209, 130, 145), (65, 73, 156), (185, 87, 195), (219, 109, 158), (133, 234, 159), (66, 72, 183), (164, 75, 143), (77, 179, 87), (104, 211, 195), (124, 245, 148), (234, 109, 123), (50, 187, 118), (173, 116, 193), (129, 190, 116), (80, 33, 245), (48, 43, 174), (117, 89, 139), (148, 33, 72), (235, 133, 158), (182, 35, 246), (218, 237, 163), (164, 228, 214), (92, 37, 129), (73, 140, 113), (233, 144, 101), (78, 181, 32), (150, 91, 110), (179, 108, 196), (189, 33, 103), (206, 171, 78), (236, 159, 80), (207, 115, 67), (115, 60, 250), (47, 178, 189), (205, 236, 77), (171, 216, 42), (183, 77, 224), (49, 189, 209), (152, 72, 93), (154, 176, 154), (84, 72, 152), (60, 94, 37), (239, 34, 72), (80, 72, 135), (81, 144, 70), (112, 36, 68), (150, 189, 233), (45, 83, 186), (127, 158, 128), (77, 248, 150), (143, 121, 83), (124, 77, 88), (102, 216, 166), (94, 55, 204), (136, 153, 185), (159, 238, 197), (30, 164, 188), (53, 119, 196), (50, 246, 120), (119, 126, 94), (149, 202, 123), (224, 183, 63), (192, 77, 197), (47, 145, 219), (228, 231, 41), (39, 202, 88), (159, 47, 141), (172, 209, 213), (59, 195, 142), (180, 148, 169), (36, 154, 169), (45, 169, 96), (145, 223, 69), (248, 59, 244), (119, 246, 186), (190, 249, 173), (204, 235, 123), (150, 188, 96), (176, 221, 188), (180, 155, 30), (74, 107, 232), (235, 191, 92), (200, 203, 198), (93, 61, 132), (204, 99, 31), (32, 200, 68), (120, 36, 113), (162, 37, 62), (92, 215, 211), (41, 200, 96), (57, 73, 82), (58, 42, 68), (148, 181, 144), (103, 227, 217), (246, 112, 243), (79, 69, 81), (83, 152, 45), (198, 195, 188), (226, 31, 134), (245, 248, 220), (176, 35, 50), (74, 113, 41), (92, 70, 87), (104, 153, 163), (221, 245, 83), (191, 158, 83), (213, 211, 183), (40, 186, 88), (40, 108, 69), (138, 111, 173), (79, 31, 220), (67, 240, 233), (106, 177, 139), (158, 140, 105), (211, 221, 32), (236, 241, 167), (114, 211, 175), (99, 221, 65), (170, 43, 83), (110, 239, 192), (130, 122, 56), (233, 151, 174), (122, 134, 62), (86, 165, 183), (230, 140, 81), (195, 204, 240), (182, 192, 240), (232, 172, 65), (234, 207, 81), (91, 67, 217), (249, 95, 154), (89, 187, 134), (45, 232, 151), (64, 102, 192), (38, 115, 206), (75, 90, 237), (100, 162, 211), (145, 118, 236), (219, 164, 63), (146, 41, 52), (82, 230, 65), (85, 233, 146), (83, 249, 183), (135, 226, 160), (217, 168, 222), (231, 159, 200), (215, 230, 73), (188, 186, 144), (111, 131, 87), (58, 134, 77), (76, 229, 193), (53, 177, 127), (155, 146, 89), (86, 54, 127), (176, 242, 225), (113, 210, 63), (147, 65, 189), (114, 236, 234), (71, 158, 149), (54, 147, 113), (228, 60, 148), (149, 162, 205), (100, 212, 109), (97, 109, 149), (111, 47, 77), (137, 72, 245), (144, 169, 42), (151, 244, 122), (232, 220, 131), (223, 113, 130), (72, 109, 193), (109, 142, 181), (175, 191, 159), (83, 54, 155), (227, 154, 55), (52, 122, 79), (223, 206, 33), (111, 105, 66), (130, 102, 105), (147, 90, 164), (205, 175, 178), (37, 64, 204), (102, 132, 133), (32, 238, 73), (137, 42, 141), (237, 78, 36), (32, 192, 176), (186, 46, 48), (152, 235, 107), (69, 61, 42), (87, 221, 144), (223, 34, 91), (250, 245, 39), (52, 184, 156), (184, 82, 189), (87, 185, 192), (74, 131, 235), (104, 196, 186), (93, 43, 73), (168, 180, 110), (66, 108, 199), (178, 63, 128), (92, 214, 122), (42, 157, 200), (73, 78, 39), (177, 152, 143), (187, 157, 176), (145, 190, 210), (154, 144, 155), (180, 48, 97), (122, 187, 218), (85, 159, 87), (154, 126, 198)]

class CrossEntropyLoss2d(nn.Module):
    def __init__(self, weight=med_frq):
        super(CrossEntropyLoss2d, self).__init__()
        self.ce_loss = nn.CrossEntropyLoss(torch.from_numpy(np.array(weight)).float(),
                                           size_average=False, reduce=False)

    def forward(self, inputs_scales, targets_scales):
        losses = []
        for inputs, targets in zip(inputs_scales, targets_scales):
            mask = targets > 0
            targets_m = targets.clone()
            targets_m[mask] -= 1
            loss_all = self.ce_loss(inputs, targets_m.long())
            losses.append(torch.sum(torch.masked_select(loss_all, mask)) / torch.sum(mask.float()))
        total_loss = sum(losses)
        return total_loss


def color_label(label):
    label = label.clone().cpu().data.numpy()
    colored_label = np.vectorize(lambda x: label_colours[int(x)])

    colored = np.asarray(colored_label(label)).astype(np.float32)
    colored = colored.squeeze()

    try:
        return torch.from_numpy(colored.transpose([1, 0, 2, 3]))
    except ValueError:
        return torch.from_numpy(colored[np.newaxis, ...])


def print_log(global_step, epoch, local_count, count_inter, dataset_size, loss, time_inter):
    print('Step: {:>5} Train Epoch: {:>3} [{:>4}/{:>4} ({:3.1f}%)]    '
          'Loss: {:.6f} [{:.2f}s every {:>4} data]'.format(
        global_step, epoch, local_count, dataset_size,
        100. * local_count / dataset_size, loss.data, time_inter, count_inter))


def save_ckpt(ckpt_dir, model, optimizer, global_step, epoch, local_count, num_train):
    # usually this happens only on the start of a epoch
    epoch_float = epoch + (local_count / num_train)
    state = {
        'global_step': global_step,
        'epoch': epoch_float,
        'state_dict': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }
    ckpt_model_filename = "ckpt_epoch_{:0.2f}.pth".format(epoch_float)
    path = os.path.join(ckpt_dir, ckpt_model_filename)
    torch.save(state, path)
    print('{:>2} has been successfully saved'.format(path))


def load_ckpt(model, optimizer, model_file, device):
    if os.path.isfile(model_file):
        print("=> loading checkpoint '{}'".format(model_file))
        if device.type == 'cuda':
            checkpoint = torch.load(model_file)
        else:
            checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)
        model.load_state_dict(checkpoint['state_dict'])
        if optimizer:
            optimizer.load_state_dict(checkpoint['optimizer'])
        print("=> loaded checkpoint '{}' (epoch {})"
              .format(model_file, checkpoint['epoch']))
        step = checkpoint['global_step']
        epoch = checkpoint['epoch']
        return step, epoch
    else:
        print("=> no checkpoint found at '{}'".format(model_file))
        os._exit(0)
